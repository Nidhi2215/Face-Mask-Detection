{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Importing the dataset \nimport numpy as np \nimport pandas as pd \nimport os     #os module is used here for fetching the content of directory containing dataset\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-27T13:24:40.112277Z","iopub.execute_input":"2022-06-27T13:24:40.112643Z","iopub.status.idle":"2022-06-27T13:24:40.140025Z","shell.execute_reply.started":"2022-06-27T13:24:40.112597Z","shell.execute_reply":"2022-06-27T13:24:40.139092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#All the libraries/modules required to build face mask detection model\nfrom bs4 import BeautifulSoup   #Beautiful soup for web scraping\nimport matplotlib.pyplot as plt\nimport os    \nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import Dropout,BatchNormalization\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam  \nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\nimport cv2\n","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:24:40.141413Z","iopub.execute_input":"2022-06-27T13:24:40.142368Z","iopub.status.idle":"2022-06-27T13:24:40.153347Z","shell.execute_reply.started":"2022-06-27T13:24:40.142320Z","shell.execute_reply":"2022-06-27T13:24:40.152442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **DATA PREPROCESSING**","metadata":{}},{"cell_type":"code","source":"#Here this function is used to locate the face in the annotation provided to make predictions.\ndef generate_box(obj):                  \n    xmin = int(obj.find('xmin').text)\n    ymin = int(obj.find('ymin').text)\n    xmax = int(obj.find('xmax').text)\n    ymax = int(obj.find('ymax').text)\n    \n    return [xmin, ymin, xmax, ymax]   #coordinates of Face in  annotation (Just like Bounding Box)\n#This function will convert categorical labels into numbers which are undertsandable by the model.\ndef generate_label(obj):  #generate_label function encodes the three classes and converts the labels into numbers.\n\n    if obj.find('name').text == \"with_mask\":\n        return 1\n    elif obj.find('name').text == \"mask_weared_incorrect\":\n        return 2\n    return 0\n#Using this generate_target function we parse the annotations file and get the objects out from them\ndef generate_target(image_id, file): \n    with open(file) as f:\n        data = f.read()       #we are reading the annotation file here in order to obtain the object under the object tag in annotation xml file.\n        soup = BeautifulSoup(data, 'xml')   #BeautifulSoup is used for scraping the annotation file which is in xml format. Here soup is the object of BeautifulSoup\n        objects = soup.find_all('object')   #find_all finds all the objects in annotation file for which the function is being called in the form of a list of strings.\n\n        num_objs = len(objects)    #num_objs is an integer variable containing the total number of objects in an image.\n\n        boxes = []\n        labels = []     \n        for i in objects:\n            boxes.append(generate_box(i)) \n            #Now  the face coordinates of objects are being appended to box list\n            labels.append(generate_label(i)) #And the associated labels(i.e masked ,not properly masked or Not masked )to labels (list).\n            \n        boxes=np.array(boxes)           #to convert the boxes(datatype:list) and labels(datatype:list) into numpy array which is accepted by a model.\n        labels=np.array(labels)         \n        img_id = np.array(image_id)     #img_id is the index of the image  in the dataset \n        target = {}\n        target[\"boxes\"] = boxes    #target is a dictionary having key as image array and values as the associated labels\n        target[\"labels\"] = labels\n        \n        return (target,num_objs)\n    '''\n    -->so basically this generate_target function has the following parameters:\n       *image_id: It is the index/number of image in the image folder of dataset.\n       *file: It is the path of the annotation file .\n    -->This function returns target which is a dictionary:{key:value}\n       * Here key :boxes(i.e face coordinates which is the region of interest in the whole image )\n       * Here value:labels associated with \n    \n    '''","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:24:40.190675Z","iopub.execute_input":"2022-06-27T13:24:40.191852Z","iopub.status.idle":"2022-06-27T13:24:40.203358Z","shell.execute_reply.started":"2022-06-27T13:24:40.191801Z","shell.execute_reply":"2022-06-27T13:24:40.202595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imgs = list(sorted(os.listdir(\"/kaggle/input/face-mask-detection/images/\")))\nlen(imgs)     #there are in total 853 images in image folder","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:24:40.204583Z","iopub.execute_input":"2022-06-27T13:24:40.205287Z","iopub.status.idle":"2022-06-27T13:24:40.223954Z","shell.execute_reply.started":"2022-06-27T13:24:40.205259Z","shell.execute_reply":"2022-06-27T13:24:40.222865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = list(sorted(os.listdir(\"/kaggle/input/face-mask-detection/annotations/\")))\nlen(labels)           #853 annotations","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:24:40.225435Z","iopub.execute_input":"2022-06-27T13:24:40.226187Z","iopub.status.idle":"2022-06-27T13:24:40.234071Z","shell.execute_reply.started":"2022-06-27T13:24:40.226156Z","shell.execute_reply":"2022-06-27T13:24:40.232854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here we use the above functions and save results in lists\ntargets=[]     #stores face coordinates\nnumobjs=[]     #stores number of faces in each image\n#run the loop for number of images we have\nfor i in range(0,853):\n    file_image = 'maksssksksss'+ str(i) + '.png'\n    file_label = 'maksssksksss'+ str(i) + '.xml'\n    img_path = os.path.join(\"/kaggle/input/face-mask-detection/images/\", file_image)  #it concatenates the path contents.\n    label_path = os.path.join(\"/kaggle/input/face-mask-detection/annotations/\", file_label)\n    #Generate Label\n    target,numobj = generate_target(i, label_path)   \n    targets.append(target)\n    numobjs.append(numobj)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:24:40.236090Z","iopub.execute_input":"2022-06-27T13:24:40.236726Z","iopub.status.idle":"2022-06-27T13:24:45.350946Z","shell.execute_reply.started":"2022-06-27T13:24:40.236692Z","shell.execute_reply":"2022-06-27T13:24:45.349999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(target)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:24:45.353954Z","iopub.execute_input":"2022-06-27T13:24:45.354518Z","iopub.status.idle":"2022-06-27T13:24:45.361350Z","shell.execute_reply.started":"2022-06-27T13:24:45.354473Z","shell.execute_reply":"2022-06-27T13:24:45.360238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(numobj)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:24:45.363968Z","iopub.execute_input":"2022-06-27T13:24:45.364802Z","iopub.status.idle":"2022-06-27T13:24:45.378830Z","shell.execute_reply.started":"2022-06-27T13:24:45.364753Z","shell.execute_reply":"2022-06-27T13:24:45.377914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numobjs[0]   #there are three faces in image 0 for which labeling has been done ","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:24:45.380476Z","iopub.execute_input":"2022-06-27T13:24:45.380726Z","iopub.status.idle":"2022-06-27T13:24:45.388226Z","shell.execute_reply.started":"2022-06-27T13:24:45.380701Z","shell.execute_reply":"2022-06-27T13:24:45.387603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(targets[:5])    #targets is a list which contains all the face coordinates and labels of all objects in each and every annotation fil","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:24:45.389244Z","iopub.execute_input":"2022-06-27T13:24:45.390000Z","iopub.status.idle":"2022-06-27T13:24:45.402052Z","shell.execute_reply.started":"2022-06-27T13:24:45.389974Z","shell.execute_reply":"2022-06-27T13:24:45.401351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# length of target is 853\nprint(targets[0])    #There are in total three faces in the first image out of which two are not wearing a mask and the third person is wearing a mask properly.  \nprint(type(targets))     ","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:24:45.402950Z","iopub.execute_input":"2022-06-27T13:24:45.403608Z","iopub.status.idle":"2022-06-27T13:24:45.411420Z","shell.execute_reply.started":"2022-06-27T13:24:45.403581Z","shell.execute_reply":"2022-06-27T13:24:45.410899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2     #For computer vision.\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import img_to_array    #for converting image format to numpy array format which is acceptible by the model\nfrom tensorflow import keras\nface_images=[]\nface_labels=[]\nfor i in range(853):\n    img_path = r\"../input/face-mask-detection/images/maksssksksss{}.png\".format(i)   #this will give the following image path:/input/face-mask-detection/images/maksssksksssi.png\n    #read image from specified file path\n    img = cv2.imread(img_path)  \n    for j in range(numobjs[i]):\n#       get coordinates of ith image in list \n        locs=(targets[i]['boxes'][j])\n#     Get the face from the image using the coordinates\n#the arguments are as ymin , ymax and xmin xmax\n        img1=img[locs[1]:locs[3],locs[0]:locs[2]]    # 0-->xmin , 1-->ymin , 2-->xmax , 3-->ymax   (with the help of this command the face is obtained in img1)\n        img1 = cv2.resize(img1, (224, 224))   # here the face obtained is resized to (224,224) pixels\n        img1 = img_to_array(img1)      #Now the image so obtained is converted into array.\n        #img1 = preprocess_input(img1) \n        face_images.append(img1)   # we have our required image in array form \n        face_labels.append(targets[i]['labels'][j])   #and these are the labels that are associated with it\n\nface_images= np.array(face_images, dtype=\"float32\")\nface_labels = np.array(face_labels)\n'''cv2.imread() method loads an image from the specified file. \nIf the image cannot be read (because of missing file, improper permissions, unsupported or invalid format) \nthen this method returns an empty matrix.'''","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:24:45.412552Z","iopub.execute_input":"2022-06-27T13:24:45.412958Z","iopub.status.idle":"2022-06-27T13:25:06.616392Z","shell.execute_reply.started":"2022-06-27T13:24:45.412933Z","shell.execute_reply":"2022-06-27T13:25:06.614908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(face_images)   #the no. of faceImages and the labels associated with it are the same","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:25:06.617764Z","iopub.execute_input":"2022-06-27T13:25:06.618130Z","iopub.status.idle":"2022-06-27T13:25:06.625128Z","shell.execute_reply.started":"2022-06-27T13:25:06.618098Z","shell.execute_reply":"2022-06-27T13:25:06.624026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(face_labels)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:25:06.626173Z","iopub.execute_input":"2022-06-27T13:25:06.626501Z","iopub.status.idle":"2022-06-27T13:25:06.639301Z","shell.execute_reply.started":"2022-06-27T13:25:06.626471Z","shell.execute_reply":"2022-06-27T13:25:06.638301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For counting the datapoints lying in each class.\nunique, counts = np.unique(face_labels, return_counts=True)    \ndict(zip(unique, counts))   \n","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:25:06.641642Z","iopub.execute_input":"2022-06-27T13:25:06.642377Z","iopub.status.idle":"2022-06-27T13:25:06.654178Z","shell.execute_reply.started":"2022-06-27T13:25:06.642348Z","shell.execute_reply":"2022-06-27T13:25:06.652596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Encode the labels in one hot encode form \nlb = LabelEncoder()\nlabels = lb.fit_transform(face_labels)\nlabels = to_categorical(labels)    \nlabels","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:25:06.655737Z","iopub.execute_input":"2022-06-27T13:25:06.656193Z","iopub.status.idle":"2022-06-27T13:25:06.671940Z","shell.execute_reply.started":"2022-06-27T13:25:06.656157Z","shell.execute_reply":"2022-06-27T13:25:06.670679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"DATA AUGMENTATION","metadata":{}},{"cell_type":"code","source":"#Perform data augmentation to increase the data for training the model.   #Here aug is the object of ImageDataGenerator.\naug = ImageDataGenerator(\n    zoom_range=0.1,\n    rotation_range=25,   \n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.15,\n    horizontal_flip=True,     \n    fill_mode=\"nearest\"    #the most imp. parameter of ImageDataGenerator\n    )","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:25:06.674389Z","iopub.execute_input":"2022-06-27T13:25:06.675714Z","iopub.status.idle":"2022-06-27T13:25:06.685843Z","shell.execute_reply.started":"2022-06-27T13:25:06.675528Z","shell.execute_reply":"2022-06-27T13:25:06.684442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INIT_LR = 1e-4     #this is the learning rate .Default learning rate is 0.001 for Adam.\nEPOCHS = 20        #The model will run for 20 times\nBS = 32            #Standard batch size is 32.","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:25:06.687409Z","iopub.execute_input":"2022-06-27T13:25:06.688691Z","iopub.status.idle":"2022-06-27T13:25:06.698727Z","shell.execute_reply.started":"2022-06-27T13:25:06.688599Z","shell.execute_reply":"2022-06-27T13:25:06.697483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport cv2\nimport matplotlib.patches as patches\nimport tensorflow as tf\nfrom keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, Dropout\nfrom keras.models import Sequential","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:25:06.701078Z","iopub.execute_input":"2022-06-27T13:25:06.701808Z","iopub.status.idle":"2022-06-27T13:25:06.712147Z","shell.execute_reply.started":"2022-06-27T13:25:06.701766Z","shell.execute_reply":"2022-06-27T13:25:06.711492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SPLITTING DATA INTO TRAIN &TEST SET","metadata":{}},{"cell_type":"code","source":"face_images","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:25:06.713172Z","iopub.execute_input":"2022-06-27T13:25:06.713514Z","iopub.status.idle":"2022-06-27T13:25:06.736426Z","shell.execute_reply.started":"2022-06-27T13:25:06.713489Z","shell.execute_reply":"2022-06-27T13:25:06.735764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#divide data into training and testing sets\n(trainX, testX, trainY, testY) = train_test_split(face_images, labels,\n\ttest_size=0.2, stratify=labels, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:25:06.737300Z","iopub.execute_input":"2022-06-27T13:25:06.738279Z","iopub.status.idle":"2022-06-27T13:25:07.178364Z","shell.execute_reply.started":"2022-06-27T13:25:06.738251Z","shell.execute_reply":"2022-06-27T13:25:07.177136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Normalize the training and testing data to get better results","metadata":{}},{"cell_type":"code","source":"# Dividingthe  train and test images by the maximum value (normalize it)\ntrainX = trainX / 255.0\ntestX = testX / 255.0\n\n# The min and max values of the training data\ntrainX.min(), trainX.max()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:25:07.179622Z","iopub.execute_input":"2022-06-27T13:25:07.180051Z","iopub.status.idle":"2022-06-27T13:25:09.124302Z","shell.execute_reply.started":"2022-06-27T13:25:07.180022Z","shell.execute_reply":"2022-06-27T13:25:09.123239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainX.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:25:09.125854Z","iopub.execute_input":"2022-06-27T13:25:09.126698Z","iopub.status.idle":"2022-06-27T13:25:09.133449Z","shell.execute_reply.started":"2022-06-27T13:25:09.126643Z","shell.execute_reply":"2022-06-27T13:25:09.132561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainY.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:25:09.134767Z","iopub.execute_input":"2022-06-27T13:25:09.135121Z","iopub.status.idle":"2022-06-27T13:25:09.144412Z","shell.execute_reply.started":"2022-06-27T13:25:09.135066Z","shell.execute_reply":"2022-06-27T13:25:09.143783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainX[0].shape","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:25:09.145203Z","iopub.execute_input":"2022-06-27T13:25:09.145578Z","iopub.status.idle":"2022-06-27T13:25:09.156983Z","shell.execute_reply.started":"2022-06-27T13:25:09.145553Z","shell.execute_reply":"2022-06-27T13:25:09.156278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainY[0].shape","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:25:09.158000Z","iopub.execute_input":"2022-06-27T13:25:09.158258Z","iopub.status.idle":"2022-06-27T13:25:09.167537Z","shell.execute_reply.started":"2022-06-27T13:25:09.158233Z","shell.execute_reply":"2022-06-27T13:25:09.167029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del targets,face_images,face_labels     #as we have no use for them","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:25:09.168346Z","iopub.execute_input":"2022-06-27T13:25:09.168991Z","iopub.status.idle":"2022-06-27T13:25:09.179278Z","shell.execute_reply.started":"2022-06-27T13:25:09.168950Z","shell.execute_reply":"2022-06-27T13:25:09.178228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BUILDING  A MODEL","metadata":{}},{"cell_type":"code","source":"# Set random seed\ntf.random.set_seed(42)    \n#Relu: if input is negative it returns a zero and if input is positive it gives the same element(i.e without any modification)\nmodel_1 = keras.models.Sequential([     #keras Sequential API has been used\n    keras.layers.Conv2D(filters = 5, kernel_size = 3, activation = 'relu', \n                        input_shape = (224,224,3)),    #filter : 5 (randomly alloted)\n    keras.layers.Conv2D(filters = 5, kernel_size = 3, activation = 'relu'),\n    keras.layers.MaxPool2D(pool_size = 2, padding = 'valid'),   #to reduce the dimension of feature map MaxPool is used which will chose the highest of element from the region of the feature map covered by the filter.\n    keras.layers.Conv2D(filters = 5, kernel_size = 3, activation = 'relu'),\n    keras.layers.Conv2D(filters = 5, kernel_size = 3, activation = 'relu'),\n    keras.layers.MaxPool2D(pool_size = 2, padding = 'valid'),\n    keras.layers.Flatten(),\n    keras.layers.Dense(units = 3, activation = 'softmax')   #units =3 because 3 classes are there: 1.Masked\n                                                            #                                      2.with mask but  incorrectluy\n])                                                          #                                      3.without mask\n\n\n'''\nThe flatten layer is used so as to convert our final feature map(after pooling) into a linear 1-D array\nas  the next layer  is the dense layer \nwhich accepts 1 D tensors only.\n'''","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:25:09.180794Z","iopub.execute_input":"2022-06-27T13:25:09.181169Z","iopub.status.idle":"2022-06-27T13:25:09.259212Z","shell.execute_reply.started":"2022-06-27T13:25:09.181143Z","shell.execute_reply":"2022-06-27T13:25:09.258374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# COMPILING AND FITTING A MODEL","metadata":{}},{"cell_type":"code","source":"opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n#helps in optimization and generalization.\nmodel_1.compile(loss=\"categorical_crossentropy\", optimizer=opt,     #as this is a multi-class classification problem thats why we have used categorical crossentropy.\n\tmetrics=[\"accuracy\"])\nH = model_1.fit(\n\taug.flow(trainX, trainY, batch_size=BS),\n\tsteps_per_epoch=len(trainX) // BS,\n\tvalidation_data=(testX, testY), \n\tvalidation_steps=len(testX) // BS,\n\tepochs=EPOCHS)\n\n#Class having high weight will be considered more importat while training","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:25:09.260299Z","iopub.execute_input":"2022-06-27T13:25:09.260588Z","iopub.status.idle":"2022-06-27T13:47:18.057344Z","shell.execute_reply.started":"2022-06-27T13:25:09.260563Z","shell.execute_reply":"2022-06-27T13:47:18.056526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\n\nplot_model(model_1, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:47:18.058645Z","iopub.execute_input":"2022-06-27T13:47:18.058937Z","iopub.status.idle":"2022-06-27T13:47:18.353998Z","shell.execute_reply.started":"2022-06-27T13:47:18.058912Z","shell.execute_reply":"2022-06-27T13:47:18.353250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:47:18.355126Z","iopub.execute_input":"2022-06-27T13:47:18.355795Z","iopub.status.idle":"2022-06-27T13:47:18.362304Z","shell.execute_reply.started":"2022-06-27T13:47:18.355758Z","shell.execute_reply":"2022-06-27T13:47:18.361556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**IMPROVING THE MODEL (TWEAKING THE PREVIOUS MODEL TO GET BETTER ACCURACY)**","metadata":{}},{"cell_type":"code","source":"# Set random seed\ntf.random.set_seed(42)\n\nmodel_2 = keras.models.Sequential([\n    keras.layers.Conv2D(filters = 10, kernel_size = 3, activation = 'relu', \n                        input_shape = (224,224,3)),\n    keras.layers.Conv2D(filters = 10, kernel_size = 3, activation = 'relu'),\n    keras.layers.MaxPool2D(pool_size = 2, padding = 'valid'),\n    keras.layers.Conv2D(filters = 10, kernel_size = 3, activation = 'relu'),\n    keras.layers.Conv2D(filters = 10,  kernel_size = 3, activation = 'relu'),\n    keras.layers.MaxPool2D(pool_size = 2, padding = 'valid'),\n    keras.layers.Flatten(),\n    keras.layers.Dense(units = 3, activation = 'softmax')  \n])","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:47:18.363624Z","iopub.execute_input":"2022-06-27T13:47:18.363940Z","iopub.status.idle":"2022-06-27T13:47:18.435663Z","shell.execute_reply.started":"2022-06-27T13:47:18.363911Z","shell.execute_reply":"2022-06-27T13:47:18.434847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS=30","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:47:18.439910Z","iopub.execute_input":"2022-06-27T13:47:18.440209Z","iopub.status.idle":"2022-06-27T13:47:18.444098Z","shell.execute_reply.started":"2022-06-27T13:47:18.440184Z","shell.execute_reply":"2022-06-27T13:47:18.443187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)    #decay rate is being used here to reduce the learning rate with each epoch.\nmodel_2.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n\tmetrics=[\"accuracy\"])\nH = model_2.fit(\n\taug.flow(trainX, trainY, batch_size=BS),\n\tsteps_per_epoch=len(trainX) // BS,   #it is the number of steps to yield before declaring one epoch finished and starting a new one.\n\tvalidation_data=(testX, testY),\n\tvalidation_steps=len(testX) // BS,\n\tepochs=EPOCHS) ","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:54:02.982446Z","iopub.execute_input":"2022-06-27T13:54:02.982808Z","iopub.status.idle":"2022-06-27T14:37:00.891856Z","shell.execute_reply.started":"2022-06-27T13:54:02.982784Z","shell.execute_reply":"2022-06-27T14:37:00.890746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\n\nplot_model(model_2, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:52:06.192223Z","iopub.status.idle":"2022-06-27T13:52:06.192960Z","shell.execute_reply.started":"2022-06-27T13:52:06.192733Z","shell.execute_reply":"2022-06-27T13:52:06.192758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:52:06.194087Z","iopub.status.idle":"2022-06-27T13:52:06.194866Z","shell.execute_reply.started":"2022-06-27T13:52:06.194639Z","shell.execute_reply":"2022-06-27T13:52:06.194678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PREDICTIONS","metadata":{}},{"cell_type":"code","source":"'''Our model outputs a list of prediction probabilities meaning,\nit outputs a number for how likely it thinks a particular class is to be the label.\nThe higher the number in the prediction probabilities list, the more likely the \nmodel believes that is the right class.'''\nProbsY = model_1.predict(testX)\nprint(ProbsY)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:52:41.527380Z","iopub.execute_input":"2022-06-27T13:52:41.527738Z","iopub.status.idle":"2022-06-27T13:52:43.514298Z","shell.execute_reply.started":"2022-06-27T13:52:41.527708Z","shell.execute_reply":"2022-06-27T13:52:43.513401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting all of the predictions from probabilities to labels\nPredictionY = ProbsY.argmax(axis=1)    \n\n# View the first 10 prediction labels\nPredictionY[:10]","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:52:52.739672Z","iopub.execute_input":"2022-06-27T13:52:52.740061Z","iopub.status.idle":"2022-06-27T13:52:52.746563Z","shell.execute_reply.started":"2022-06-27T13:52:52.740031Z","shell.execute_reply":"2022-06-27T13:52:52.746063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#These are the classes\nclass_names=[\"without_mask\",\"with_mask\",\"mask_weared_incorrect\"]   ","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:53:20.856533Z","iopub.execute_input":"2022-06-27T13:53:20.857571Z","iopub.status.idle":"2022-06-27T13:53:20.862053Z","shell.execute_reply.started":"2022-06-27T13:53:20.857535Z","shell.execute_reply":"2022-06-27T13:53:20.861105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See the predicted class number and associated label\nProbsY[20].argmax(), class_names[ProbsY[20].argmax()]","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:53:31.776505Z","iopub.execute_input":"2022-06-27T13:53:31.777174Z","iopub.status.idle":"2022-06-27T13:53:31.782750Z","shell.execute_reply.started":"2022-06-27T13:53:31.777147Z","shell.execute_reply":"2022-06-27T13:53:31.782125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MODEL_1","metadata":{}},{"cell_type":"code","source":"print(\"[INFO] evaluating network...\")\npredIdxs = model_1.predict(testX, batch_size=32)\n\n# for each image in the testing set we need to find the index of the\n# label with corresponding largest predicted probability\npredIdxs = np.argmax(predIdxs, axis=1)\n\n# show a nicely formatted classification report\nprint(classification_report(testY.argmax(axis=1), predIdxs\n\t))\n\n# # serialize the model to disk\n# print(\"[INFO] saving mask detector model...\")\n\n# plot the training loss and accuracy\nN = EPOCHS\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\nplt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss/Accuracy\")\nplt.legend(loc=\"lower left\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T14:37:03.017091Z","iopub.execute_input":"2022-06-27T14:37:03.017312Z","iopub.status.idle":"2022-06-27T14:37:05.477677Z","shell.execute_reply.started":"2022-06-27T14:37:03.017289Z","shell.execute_reply":"2022-06-27T14:37:05.476711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MODEL_2","metadata":{}},{"cell_type":"code","source":"print(\"[INFO] evaluating network...\")\npredIdxs = model_2.predict(testX, batch_size=32)\n\n# for each image in the testing set we need to find the index of the\n# label with corresponding largest predicted probability\npredIdxs = np.argmax(predIdxs, axis=1)\n\n# show a nicely formatted classification report\nprint(classification_report(testY.argmax(axis=1), predIdxs\n\t))\n\n# # serialize the model to disk\n# print(\"[INFO] saving mask detector model...\")\n\n# plot the training loss and accuracy\nN = EPOCHS\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\nplt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss/Accuracy\")\nplt.legend(loc=\"lower left\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T13:52:06.206971Z","iopub.status.idle":"2022-06-27T13:52:06.207473Z","shell.execute_reply.started":"2022-06-27T13:52:06.207294Z","shell.execute_reply":"2022-06-27T13:52:06.207314Z"},"trusted":true},"execution_count":null,"outputs":[]}]}